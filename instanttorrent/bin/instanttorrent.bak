#!/usr/bin/python3

import os
import sys
import re
from argparse import ArgumentParser

import requests
from bs4 import BeautifulSoup

parser = ArgumentParser()
parser.add_argument('-m', '--max_results', help='max results to output', type=int, default=10)
parser.add_argument('-q', '--query', help='query to be searched', type=str, default='')
parser.add_argument('-p', '--proxy', help='proxy to access TPB', type=str, default=None)
parser.add_argument('-t','--title', help='show title', action='store_true', default=True)
parser.add_argument('-s','--seeders', help='show seeders', action='store_true', default=True)
parser.add_argument('-l','--leechers', help='show leechers', action='store_true', default=True)
parser.add_argument('-d','--date', help='show upload date', action='store_true', default=False)
parser.add_argument('--size', help='show file size', action='store_true', default=False)
args = parser.parse_args()

if (sys.version_info < (3,)):
    print('Please run this in python3, python2 is not supported')
    raise SystemExit

class InstantTorrent:
    def __init__(self, proxy):
        self._proxy = self.setup_proxy(proxy)
        self._session = requests.Session()
        self._session.proxies = self._proxy

    def setup_proxy(self, proxy):
        if proxy is None:
            http_proxy = os.environ.get('HTTP_PROXY')
            https_proxy = os.environ.get('HTTPS_PROXY')
            ftp_proxy = os.environ.get('FTP_PROXY')
            return {
                'http': http_proxy,
                'https': https_proxy,
                'ftp': ftp_proxy
            }
        return {
            'http': 'http://' + proxy,
            'https': 'https://' + proxy,
            'ftp': 'ftp://' + proxy
        }

    def output(self, results, i):
        '''
        :param results: array/list
        '''
        s = ''
        if args.title is True:
            s = '{}'.format(results[1])
            if i == 0:
                print('  Title  | ',end='')
        if args.seeders is True:
            s = '{}  {}'.format(s, results[2])
            if i == 0:
                print('  Seeders  | ', end='')
        if args.leechers is True:
            s = '{}  {}'.format(s, results[3])
            if i == 0:
                print('  Leechers  | ',end='')
        if args.date is True:
            s = '{}  {}'.format(s, results[4])
            if i == 0:
                print('  Date  | ', end='')
        if args.size is True:
            s = '{}  {}'.format(s, results[5])
            if i == 0:
                 print('  Size    ', end='')
        print('\r\r\r\r', end='')
        if i == 0:
            print('\n')
        try:
            print('[{}] {}'.format(i, s))
        except UnicodeEncodeError:
            print('[{}] {}'.format(i, s.encode('utf8').decode(sys.stdout.encoding)))
            # Python3 encoding fix

    def parse(self, html):
        results = []
        html = BeautifulSoup(html, 'html.parser').findAll('tr')
        def title(html):
            title = html.find(class_='detName').find('a').get_text()
            return title
        def url(html):
            url = html.find(class_='detName').find('a')['href']
            url = 'https://thepiratebay.org' + url
            return url
        def seeders(html):
            seeders = html.findAll('td')[-2:][0].get_text()
            return seeders
        def leechers(html):
            leechers = html.findAll('td')[-2:][1].get_text()
            return leechers
        def date(html):
            date = html.find(class_='detDesc').get_text()
            date = date.split(',')[0].replace('Uploaded ','')
            return date
        def size(html):
            size = html.find(class_='detDesc').get_text()
            size = re.search(r'Size [0-9]+.[0-9]+....', size).group().replace('\xa0','')
            return size
        for tag in html:
            try:
                results.append([url(tag), title(tag), seeders(tag), leechers(tag), date(tag), size(tag)])
            except Exception as e:
                print(e)
        return results

    def search(self, query):
        url = 'https://thepiratebay.org/search/{}/0/99/0'.format(query)
        r = self._session.get(url)
        results = self.parse(r.content)
        for i, result in enumerate(results):
            if i > args.max_results:
                break
            self.output(result, i)
            #self.output(self.get_stats('https://thepiratebay.org' + results[n].find('a')['href']))
        try:
            prompt = int(input('> '))
            result = 'https://thepiratebay.org' + results[prompt].find('a')['href']
        except (ValueError, IndexError):
            raise SystemExit
        return result

    def get_magnet_link(self, result):
        r = self._session.get(result)
        return BeautifulSoup(r.content, 'html.parser').find(class_='download').find('a')['href']

    def open_torrent(self, magnet_link):
        if os.name == 'nt':
            os.startfile(magnet_link)  # Windows
        else:
            from subprocess import call
            call(['xdg-open', magnet_link])  # Linux

if __name__ == '__main__':
    bot = InstantTorrent(args.proxy)
    query = args.query
    if query == '':
        print('enter your search query')
        query = input('> ')
    result = bot.search(query)
    magnet_link = bot.get_magnet_link(result)
    bot.open_torrent(magnet_link)

